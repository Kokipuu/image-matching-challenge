{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import csv\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# Check that you're using a recent OpenCV version.\n",
    "assert cv2.__version__ > '4.5', 'Please use OpenCV 4.5 or later.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 与えられたカメラの内部構造（キャリブレーション行列 K）および外部構造（回転行列 R，並進ベクトル T）を含む，名前付きタプル．\n",
    "Gt = namedtuple('Gt', ['K', 'R', 'T'])\n",
    "\n",
    "# A small epsilon.\n",
    "eps = 1e-15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covisibilityデータを読み込む関数\n",
    "def ReadCovisibilityData(filename):\n",
    "\n",
    "    covisibility_dict = {}\n",
    "\n",
    "    with open(filename) as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        \n",
    "        for i, row in enumerate(reader):\n",
    "            # Skip header.\n",
    "            if i == 0:\n",
    "                continue\n",
    "            covisibility_dict[row[0]] = float(row[1])\n",
    "\n",
    "    return covisibility_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴検出器のデータ構造\n",
    "# keypointsを正規化する関数\n",
    "\n",
    "def NormalizeKeypoints(keypoints, K):\n",
    "\n",
    "    C_x = K[0, 2]\n",
    "    C_y = K[1, 2]\n",
    "    f_x = K[0, 0]\n",
    "    f_y = K[1, 1]\n",
    "\n",
    "    keypoints = (keypoints - np.array([[C_x, C_y]])) / np.array([[f_x, f_y]])\n",
    "\n",
    "    return keypoints\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# キャリブレーション行列が与えられたら、基礎行列(F)から基本行列(E)を計算する.  (E=K2.T*F*K1)\n",
    "# 今回のコンペでは, Fを推定することに注意. \n",
    "\n",
    "def ComputeEssentialMatrix(F, K1, K2, kp1, kp2):\n",
    "    \n",
    "    assert F.shape[0] == 3, 'Malformed F?'\n",
    "\n",
    "    E = np.matmul(np.matmul(K2.T, F), K1).astype(np.float64)\n",
    "    \n",
    "    kp1n = NormalizeKeypoints(kp1, K1)\n",
    "    kp2n = NormalizeKeypoints(kp2, K2)\n",
    "    num_inliers, R, T, mask = cv2.recoverPose(E, kp1n, kp2n)\n",
    "\n",
    "    return E, R, T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCV のkeypointsを，単純な numpy の配列に変換する関数\n",
    "def ArrayFromCvKps(kps):\n",
    "    \n",
    "    return np.array([kp.pt for kp in kps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回転行列をクォータニオンに変換\n",
    "def QuaternionFromMatrix(matrix):\n",
    "\n",
    "    M = np.array(matrix, dtype=np.float64, copy=False)[:4, :4]\n",
    "    m00 = M[0, 0]\n",
    "    m01 = M[0, 1]\n",
    "    m02 = M[0, 2]\n",
    "    m10 = M[1, 0]\n",
    "    m11 = M[1, 1]\n",
    "    m12 = M[1, 2]\n",
    "    m20 = M[2, 0]\n",
    "    m21 = M[2, 1]\n",
    "    m22 = M[2, 2]\n",
    "\n",
    "    K = np.array([[m00 - m11 - m22, 0.0, 0.0, 0.0],\n",
    "              [m01 + m10, m11 - m00 - m22, 0.0, 0.0],\n",
    "              [m02 + m20, m12 + m21, m22 - m00 - m11, 0.0],\n",
    "              [m21 - m12, m02 - m20, m10 - m01, m00 + m11 + m22]])\n",
    "    K /= 3.0\n",
    "\n",
    "    # The quaternion is the eigenvector of K that corresponds to the largest eigenvalue.\n",
    "    w, V = np.linalg.eigh(K)\n",
    "    q = V[[3, 0, 1, 2], np.argmax(w)]\n",
    "\n",
    "    if q[0] < 0:\n",
    "        np.negative(q, q)\n",
    "\n",
    "    return q\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 与えられた画像からSIFT特徴量を計算する関数\n",
    "def ExtractSiftFeatures(image, detector, num_features):\n",
    "    \n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    kp, desc = detector.detectAndCompute(gray, None)\n",
    "    \n",
    "    return kp[:num_features], desc[:num_features]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一つの例について誤差マトリクスを計算する関数\n",
    "# 回転と並進の2つの誤差を返す．\n",
    "# これらは，mean Average Accuracy(mAA)を計算するために，ComputeMaaによって異なる閾値で結合される．\n",
    "\n",
    "def ComputeErrorForOneExample(q_gt, T_gt, q, T, scale):\n",
    "    \n",
    "    q_gt_norm = q_gt / (np.linalg.norm(q_gt) + eps)\n",
    "    q_norm = q / (np.linalg.norm(q) + eps)\n",
    "\n",
    "    loss_q = np.maximum(eps, (1.0 - np.sum(q_norm * q_gt_norm)**2))\n",
    "    err_q = np.arccos(1 - 2 * loss_q)\n",
    "\n",
    "    # このシーンにスケーリングファクターを適用する.\n",
    "    T_gt_scaled = T_gt * scale\n",
    "    T_scaled = T * np.linalg.norm(T_gt) * scale / (np.linalg.norm(T) + eps)\n",
    "\n",
    "    err_t = min(np.linalg.norm(T_gt_scaled - T_scaled), np.linalg.norm(T_gt_scaled + T_scaled))\n",
    "\n",
    "    return err_q * 180 / np.pi, err_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1つのシーンについて、異なる閾値におけるmAAを計算する関数\n",
    "def ComputeMaa(err_q, err_t, thresholds_q, thresholds_t):\n",
    "    \n",
    "    assert len(err_q) == len(err_t)\n",
    "    \n",
    "    acc, acc_q, acc_t = [], [], []\n",
    "    for th_q, th_t in zip(thresholds_q, thresholds_t):\n",
    "        acc += [(np.bitwise_and(np.array(err_q) < th_q, np.array(err_t) < th_t)).sum() / len(err_q)]\n",
    "        acc_q += [(np.array(err_q) < th_q).sum() / len(err_q)]\n",
    "        acc_t += [(np.array(err_t) < th_t).sum() / len(err_t)]\n",
    "        \n",
    "    return np.mean(acc), np.array(acc), np.array(acc_q), np.array(acc_t)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# サイズの異なる2枚の画像を重ねる関数\n",
    "def BuildCompositeImage(im1, im2, axis=1, margin=0, background=1):\n",
    "    \n",
    "    if background != 0 and background != 1:\n",
    "        background = 1\n",
    "    if axis != 0 and axis != 1:\n",
    "        raise RuntimeError('Axis must be 0 (vertical) or 1 (horizontal')\n",
    "\n",
    "    h1, w1, _ = im1.shape\n",
    "    h2, w2, _ = im2.shape\n",
    "\n",
    "    if axis == 1:\n",
    "        composite = np.zeros((max(h1, h2), w1 + w2 + margin, 3), dtype=np.uint8) + 255 * background\n",
    "        if h1 > h2:\n",
    "            voff1, voff2 = 0, (h1 - h2) // 2\n",
    "        else:\n",
    "            voff1, voff2 = (h2 - h1) // 2, 0\n",
    "        hoff1, hoff2 = 0, w1 + margin\n",
    "        \n",
    "    else:\n",
    "        composite = np.zeros((h1 + h2 + margin, max(w1, w2), 3), dtype=np.uint8) + 255 * background\n",
    "        if w1 > w2:\n",
    "            hoff1, hoff2 = 0, (w1 - w2) // 2\n",
    "        else:\n",
    "            hoff1, hoff2 = (w2 - w1) // 2, 0\n",
    "        voff1, voff2 = 0, h1 + margin\n",
    "    composite[voff1:voff1 + h1, hoff1:hoff1 + w1, :] = im1\n",
    "    composite[voff2:voff2 + h2, hoff2:hoff2 + w2, :] = im2\n",
    "\n",
    "    return (composite, (voff1, voff2), (hoff1, hoff2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keypointsとmatchesを描画する関数\n",
    "def DrawMatches(im1, im2, kp1, kp2, matches, axis=1, margin=0, background=0, linewidth=2):\n",
    "    \n",
    "    composite, v_offset, h_offset = BuildCompositeImage(im1, im2, axis, margin, background)\n",
    "\n",
    "    # Draw all keypoints.\n",
    "    for coord_a, coord_b in zip(kp1, kp2):\n",
    "        composite = cv2.drawMarker(composite, (int(coord_a[0] + h_offset[0]), int(coord_a[1] + v_offset[0])), color=(255, 0, 0), markerType=cv2.MARKER_CROSS, markerSize=5, thickness=1)\n",
    "        composite = cv2.drawMarker(composite, (int(coord_b[0] + h_offset[1]), int(coord_b[1] + v_offset[1])), color=(255, 0, 0), markerType=cv2.MARKER_CROSS, markerSize=5, thickness=1)\n",
    "    \n",
    "    # Draw matches, and highlight keypoints used in matches.\n",
    "    for idx_a, idx_b in matches:\n",
    "        composite = cv2.drawMarker(composite, (int(kp1[idx_a, 0] + h_offset[0]), int(kp1[idx_a, 1] + v_offset[0])), color=(0, 0, 255), markerType=cv2.MARKER_CROSS, markerSize=12, thickness=1)\n",
    "        composite = cv2.drawMarker(composite, (int(kp2[idx_b, 0] + h_offset[1]), int(kp2[idx_b, 1] + v_offset[1])), color=(0, 0, 255), markerType=cv2.MARKER_CROSS, markerSize=12, thickness=1)\n",
    "        composite = cv2.line(composite,\n",
    "                             tuple([int(kp1[idx_a][0] + h_offset[0]),\n",
    "                                   int(kp1[idx_a][1] + v_offset[0])]),\n",
    "                             tuple([int(kp2[idx_b][0] + h_offset[1]),\n",
    "                                   int(kp2[idx_b][1] + v_offset[1])]), color=(0, 0, 255), thickness=1)\n",
    "    return composite\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csvファイルからキャリブレーションデータ（ground truth）を読み込む関数\n",
    "def LoadCalibration(filename):\n",
    "    \n",
    "    calib_dict = {}\n",
    "    with open(filename, 'r') as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        for i, row in enumerate(reader):\n",
    "            # Skip header.\n",
    "            if i == 0:\n",
    "                continue\n",
    "\n",
    "            camera_id = row[0]\n",
    "            K = np.array([float(v) for v in row[1].split(' ')]).reshape([3, 3])\n",
    "            R = np.array([float(v) for v in row[2].split(' ')]).reshape([3, 3])\n",
    "            T = np.array([float(v) for v in row[3].split(' ')])\n",
    "            calib_dict[camera_id] = Gt(K=K, R=R, T=T)\n",
    "    \n",
    "    return calib_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = 'PATH'\n",
    "\n",
    "val_scenes = []\n",
    "\n",
    "for f in os.scandir(src):\n",
    "    if f.is_dir():\n",
    "        cur_scene = os.path.split(f)[-1]\n",
    "        print(f'Found scene \"{cur_scene}\"\" at {f.path}')\n",
    "        val_scenes += [cur_scene]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#検証セットの各シーンには、画像、ポーズ、ペアのリストが含まれています。一つを選んで、いくつかの画像を見てみましょう。\n",
    "\n",
    "scene = 'piazza_san_marco'\n",
    "\n",
    "images_dict = {}\n",
    "for filename in glob(f'{src}/{scene}/images/*.jpg'):\n",
    "    cur_id = os.path.basename(os.path.splitext(filename)[0])\n",
    "\n",
    "    # OpenCVはBGRを想定しているが，画像は標準RGBでエンコードされているので，OpenCVをI/Oに使う場合は色変換を行う必要がある\n",
    "    images_dict[cur_id] = cv2.cvtColor(cv2.imread(filename), cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "print(f'Loaded {len(images_dict)} images.')\n",
    "\n",
    "num_rows = 6\n",
    "num_cols = 4\n",
    "f, axes = plt.subplots(num_rows, num_cols, figsize=(20, 20), constrained_layout=True)\n",
    "\n",
    "for i, key in enumerate(images_dict):\n",
    "    if i >= num_rows * num_cols:\n",
    "        break\n",
    "\n",
    "    cur_ax = axes[i % num_rows, i // num_rows]\n",
    "    cur_ax.imshow(images_dict[key])\n",
    "    cur_ax.set_title(key)\n",
    "    cur_ax.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "covisibility_dict = ReadCovisibilityData(f'{src}/{scene}/pair_covisibility.csv')\n",
    "\n",
    "# 簡単なペアから見ていき、難しいペアは後で見てく\n",
    "easy_subset = [k for k, v in covisibility_dict.items() if v >= 0.7]\n",
    "difficult_subset = [k for k, v in covisibility_dict.items() if v >= 0.1 and v < 0.2]\n",
    "\n",
    "for i, subset in enumerate([easy_subset, difficult_subset]):\n",
    "    print(f'Pairs from an {\"easy\" if i == 0 else \"difficult\"} subset')\n",
    "    \n",
    "    for pair in subset[:4]:\n",
    "        # A pair string is simply two concatenated image IDs, separated with a hyphen.\n",
    "        image_id_1, image_id_2 = pair.split('-')\n",
    "\n",
    "        f, axes = plt.subplots(1, 2, figsize=(15, 10), constrained_layout=True)\n",
    "        axes[0].imshow(images_dict[image_id_1])\n",
    "        axes[0].set_title(image_id_1)\n",
    "        axes[1].imshow(images_dict[image_id_2])\n",
    "        axes[1].set_title(image_id_2)\n",
    "        for ax in axes:\n",
    "            ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "fig = plt.figure(figsize=(15, 10), constrained_layout=True)\n",
    "plt.title('Covisibility histogram')\n",
    "plt.hist(list(covisibility_dict.values()), bins=10, range=[0, 1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 5000\n",
    "\n",
    "# 小さな画像では予算が足りないので、検出しきい値を下げるとよい\n",
    "# 1つの点の特徴が複数の方向を持つことがあるので、実際には num_features 個以上の特徴が得られるかもしれないことに注意してください (これは稀なことです)。\n",
    "sift_detector = cv2.SIFT_create(num_features, contrastThreshold=-10000, edgeThreshold=-10000)\n",
    "\n",
    "keys = list(images_dict.keys())\n",
    "keypoints, descriptors = ExtractSiftFeatures(images_dict[keys[0]], sift_detector, num_features)\n",
    "print(f'Computed {len(keypoints)} features.')\n",
    "\n",
    "# Each local feature contains a keypoint (xy, possibly scale, possibly orientation) and a description vector (128-dimensional for SIFT).\n",
    "image_with_keypoints = cv2.drawKeypoints(images_dict[keys[0]], keypoints, outImage=None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "plt.imshow(image_with_keypoints)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2つの画像の局所的な特徴を総当り的に照合することで、対応関係を見つけることができます。簡単なペアでやってみましょう\n",
    "\n",
    "pair = easy_subset[0]\n",
    "image_id_1, image_id_2 = pair.split('-')\n",
    "keypoints_1, descriptors_1 = ExtractSiftFeatures(images_dict[image_id_1], sift_detector, 2000)\n",
    "keypoints_2, descriptors_2 = ExtractSiftFeatures(images_dict[image_id_2], sift_detector, 2000)\n",
    "\n",
    "bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "\n",
    "# Compute matches.\n",
    "cv_matches = bf.match(descriptors_1, descriptors_2)\n",
    "\n",
    "# Convert keypoints and matches to something more human-readable.\n",
    "cur_kp_1 = ArrayFromCvKps(keypoints_1)\n",
    "cur_kp_2 = ArrayFromCvKps(keypoints_2)\n",
    "matches = np.array([[m.queryIdx, m.trainIdx] for m in cv_matches])\n",
    "\n",
    "# Plot the brute-force matches.\n",
    "im_matches = DrawMatches(images_dict[image_id_1], images_dict[image_id_2], cur_kp_1, cur_kp_2, matches)\n",
    "fig = plt.figure(figsize=(25, 25))\n",
    "plt.title('Matches before RANSAC')\n",
    "plt.imshow(im_matches)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "F, inlier_mask = cv2.findFundamentalMat(cur_kp_1[matches[:, 0]], cur_kp_2[matches[:, 1]], cv2.USAC_MAGSAC, ransacReprojThreshold=0.25, confidence=0.99999, maxIters=10000)\n",
    "\n",
    "matches_after_ransac = np.array([match for match, is_inlier in zip(matches, inlier_mask) if is_inlier])\n",
    "im_inliers = DrawMatches(images_dict[image_id_1], images_dict[image_id_2], cur_kp_1, cur_kp_2, matches_after_ransac)\n",
    "fig = plt.figure(figsize=(25, 25))\n",
    "plt.title('Matches before RANSAC')\n",
    "plt.imshow(im_inliers)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_dict = LoadCalibration(f'{src}/{scene}/calibration.csv')\n",
    "print(f'Loded ground truth data for {len(calib_dict)} images')\n",
    "print()\n",
    "\n",
    "\n",
    "scaling_dict = {}\n",
    "with open(f'{src}/scaling_factors.csv') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    for i, row in enumerate(reader):\n",
    "        # Skip header.\n",
    "        if i == 0:\n",
    "            continue\n",
    "        scaling_dict[row[0]] = float(row[1])\n",
    "\n",
    "print(f'Scaling factors: {scaling_dict}')\n",
    "print()\n",
    "\n",
    "# これで誤差を計算することができる。まず、先ほど推定した基礎行列(F)を分解してみる. TODO explain why we do this.\n",
    "inlier_kp_1 = ArrayFromCvKps([kp for i, kp in enumerate(keypoints_1) if i in matches_after_ransac[:, 0]])\n",
    "inlier_kp_2 = ArrayFromCvKps([kp for i, kp in enumerate(keypoints_2) if i in matches_after_ransac[:, 1]])\n",
    "E, R, T = ComputeEssentialMatrix(F, calib_dict[image_id_1].K, calib_dict[image_id_2].K, inlier_kp_1, inlier_kp_2)\n",
    "q = QuaternionFromMatrix(R)\n",
    "T = T.flatten()\n",
    "\n",
    "# Get the ground truth relative pose difference for this pair of images.\n",
    "R1_gt, T1_gt = calib_dict[image_id_1].R, calib_dict[image_id_1].T.reshape((3, 1))\n",
    "R2_gt, T2_gt = calib_dict[image_id_2].R, calib_dict[image_id_2].T.reshape((3, 1))\n",
    "dR_gt = np.dot(R2_gt, R1_gt.T)\n",
    "dT_gt = (T2_gt - np.dot(dR_gt, T1_gt)).flatten()\n",
    "q_gt = QuaternionFromMatrix(dR_gt)\n",
    "q_gt = q_gt / (np.linalg.norm(q_gt) + eps)\n",
    "\n",
    "# Given ground truth and prediction, compute the error for the example above.\n",
    "err_q, err_t = ComputeErrorForOneExample(q_gt, dT_gt, q, T, scaling_dict[scene])\n",
    "print(f'Pair \"{pair}, rotation_error={err_q:.02f} (deg), translation_error={err_t:.02f} (m)', flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images = True\n",
    "num_show_images = 1\n",
    "max_pairs_per_scene = 50\n",
    "verbose = True\n",
    "\n",
    "# We use two different sets of thresholds over rotation and translation. Do not change this -- these are the values used by the scoring back-end.\n",
    "thresholds_q = np.linspace(1, 10, 10)\n",
    "thresholds_t = np.geomspace(0.2, 5, 10)\n",
    "\n",
    "# Save the per-sample errors and the accumulated metric to dictionaries, for later inspection.\n",
    "errors = {scene: {} for scene in scaling_dict.keys()}\n",
    "mAA = {scene: {} for scene in scaling_dict.keys()}\n",
    "\n",
    "# Instantiate the matcher.\n",
    "bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "\n",
    "for scene in scaling_dict.keys():\n",
    "    # Load all pairs, find those with a co-visibility over 0.1, and subsample them.\n",
    "    covisibility_dict = ReadCovisibilityData(f'{src}/{scene}/pair_covisibility.csv')    \n",
    "    pairs = [pair for pair, covis in covisibility_dict.items() if covis >= 0.1]\n",
    "    \n",
    "    print(f'-- Processing scene \"{scene}\": found {len(pairs)} pairs (will keep {min(len(pairs), max_pairs_per_scene)})', flush=True)\n",
    "    \n",
    "    # Subsample the pairs. Note that they are roughly sorted by difficulty (easy ones first), so we shuffle them beforehand: results would be misleading otherwise.\n",
    "    random.shuffle(pairs)\n",
    "    pairs = pairs[:max_pairs_per_scene]\n",
    "    \n",
    "    # Extract the images in these pairs (we don't need to load images we will not use).\n",
    "    ids = []\n",
    "    for pair in pairs:\n",
    "        cur_ids = pair.split('-')\n",
    "        assert cur_ids[0] > cur_ids[1]\n",
    "        ids += cur_ids\n",
    "    ids = list(set(ids))\n",
    "    \n",
    "    # Load ground truth data.\n",
    "    calib_dict = LoadCalibration(f'{src}/{scene}/calibration.csv')\n",
    "\n",
    "     # Load images and extract SIFT features.\n",
    "    images_dict = {}\n",
    "    kp_dict = {}\n",
    "    desc_dict = {}\n",
    "    print('Extracting features...')\n",
    "    for id in tqdm(ids):\n",
    "        images_dict[id] = cv2.cvtColor(cv2.imread(f'{src}/{scene}/images/{id}.jpg'), cv2.COLOR_BGR2RGB)\n",
    "        kp_dict[id], desc_dict[id] = ExtractSiftFeatures(images_dict[id], sift_detector, 2000)\n",
    "    print()\n",
    "    print(f'Extracted features for {len(kp_dict)} images (avg: {np.mean([len(v) for v in desc_dict.values()])})')\n",
    "\n",
    "    # Process the pairs.\n",
    "    max_err_acc_q_new = []\n",
    "    max_err_acc_t_new = []\n",
    "    for counter, pair in enumerate(pairs):\n",
    "        id1, id2 = pair.split('-')\n",
    "\n",
    "        # Compute matches by brute force.\n",
    "        cv_matches = bf.match(desc_dict[id1], desc_dict[id2])\n",
    "        matches = np.array([[m.queryIdx, m.trainIdx] for m in cv_matches])\n",
    "        cur_kp_1 = ArrayFromCvKps([kp_dict[id1][m[0]] for m in matches])\n",
    "        cur_kp_2 = ArrayFromCvKps([kp_dict[id2][m[1]] for m in matches])\n",
    "\n",
    "        # Filter matches with RANSAC.\n",
    "        F, inlier_mask = cv2.findFundamentalMat(cur_kp_1, cur_kp_2, cv2.USAC_MAGSAC, 0.25, 0.99999, 10000)\n",
    "        inlier_mask = inlier_mask.astype(bool).flatten()\n",
    "        \n",
    "        matches_after_ransac = np.array([match for match, is_inlier in zip(matches, inlier_mask) if is_inlier])\n",
    "        inlier_kp_1 = ArrayFromCvKps([kp_dict[id1][m[0]] for m in matches_after_ransac])\n",
    "        inlier_kp_2 = ArrayFromCvKps([kp_dict[id2][m[1]] for m in matches_after_ransac])\n",
    "\n",
    "        # Compute the essential matrix.\n",
    "        E, R, T = ComputeEssentialMatrix(F, calib_dict[id1].K, calib_dict[id2].K, inlier_kp_1, inlier_kp_2)\n",
    "        q = QuaternionFromMatrix(R)\n",
    "        T = T.flatten()\n",
    "\n",
    "        # Get the relative rotation and translation between these two cameras, given their R and T in the global reference frame.\n",
    "        R1_gt, T1_gt = calib_dict[id1].R, calib_dict[id1].T.reshape((3, 1))\n",
    "        R2_gt, T2_gt = calib_dict[id2].R, calib_dict[id2].T.reshape((3, 1))\n",
    "        dR_gt = np.dot(R2_gt, R1_gt.T)\n",
    "        dT_gt = (T2_gt - np.dot(dR_gt, T1_gt)).flatten()\n",
    "        q_gt = QuaternionFromMatrix(dR_gt)\n",
    "        q_gt = q_gt / (np.linalg.norm(q_gt) + eps)\n",
    "\n",
    "        # Compute the error for this example.\n",
    "        err_q, err_t = ComputeErrorForOneExample(q_gt, dT_gt, q, T, scaling_dict[scene])\n",
    "        errors[scene][pair] = [err_q, err_t]\n",
    "\n",
    "        # Plot the resulting matches and the pose error.\n",
    "        if verbose or (show_images and counter < num_show_images):\n",
    "            print(f'{pair}, err_q={(err_q):.02f} (deg), err_t={(err_t):.02f} (m)', flush=True)\n",
    "        if show_images and counter < num_show_images:\n",
    "            im_inliers = DrawMatches(images_dict[id1], images_dict[id2], ArrayFromCvKps(kp_dict[id1]), ArrayFromCvKps(kp_dict[id2]), matches_after_ransac)\n",
    "            fig = plt.figure(figsize=(25, 25))\n",
    "            plt.title(f'Inliers, \"{pair}\"')\n",
    "            plt.imshow(im_inliers)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            print()\n",
    "\n",
    "    # Histogram the errors over this scene.\n",
    "    mAA[scene] = ComputeMaa([v[0] for v in errors[scene].values()], [v[1] for v in errors[scene].values()], thresholds_q, thresholds_t)\n",
    "    print()\n",
    "    print(f'Mean average Accuracy on \"{scene}\": {mAA[scene][0]:.05f}')\n",
    "    print()\n",
    "\n",
    "print()\n",
    "print('------- SUMMARY -------')\n",
    "print()\n",
    "for scene in scaling_dict.keys():\n",
    "    print(f'-- Mean average Accuracy on \"{scene}\": {mAA[scene][0]:.05f}')\n",
    "print()\n",
    "print(f'Mean average Accuracy on dataset: {np.mean([mAA[scene][0] for scene in mAA]):.05f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
